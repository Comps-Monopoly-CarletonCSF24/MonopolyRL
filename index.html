<!DOCTYPE html>
<!--- The formatting and script of this webpage is generated by Deepseek V3.-->>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Senior Thesis on Reinforcement Learning in Monopoly using Q-learning variants. Explore our methods, results, and play against our Deep Q-Lambda agent.">
    <meta name="keywords" content="Reinforcement Learning, Q-learning, Monopoly, Deep Q-Lambda, AI, Carleton College">
    <meta name="author" content="Albert Jing, Dake Peng, Paul Claudel Izabayo, Xiaoying Qu">
    <title>Reinforcement Learning in Monopoly Through Three Q-learning Variants</title>
    <style>
        /* General Styles */
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
            color: #333;
        }

        .container {
            width: 80%;
            margin: auto;
            overflow: hidden;
        }

        /* Header */
        header {
            background: #333;
            color: #fff;
            padding-top: 30px;
            min-height: 70px;
            border-bottom: #77aaff 3px solid;
        }

        header h1 {
            text-align: center;
            margin: 0;
            font-size: 36px;
        }

        header p {
            text-align: center;
            font-size: 18px;
        }

        /* Navigation */
        nav {
            background: #444;
            color: #fff;
            padding: 10px 0;
        }

        nav ul {
            padding: 0;
            list-style: none;
            text-align: center;
        }

        nav ul li {
            display: inline;
            margin: 0 20px;
        }

        nav ul li a {
            color: #fff;
            text-decoration: none;
            font-size: 18px;
        }

        /* Sections */
        section {
            padding: 20px 0;
        }

        section h2 {
            font-size: 28px;
            margin-bottom: 10px;
        }

        section p {
            font-size: 18px;
        }
        section ul {
            font-size: 18px;
        }
        section img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px 0;
        }

        /* Footer */
        footer {
            background: #333;
            color: #fff;
            text-align: center;
            padding: 10px 0;
            margin-top: 20px;
        }

        footer a {
            color: #77aaff;
            text-decoration: none;
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>Reinforcement Learning in Monopoly Through Three Q-learning Variants</h1>
            <p>By Albert Jing, Dake Peng, Paul Claudel Izabayo, and Xiaoying Qu</p>
            <p>A 2025 Computer Science Senior Thesis (Comps) Project  at Carleton College, MN, USA</p>
        </div>
    </header>

    <nav>
        <div class="container">
            <ul>
                <li><a href="#introduction">Introduction</a></li>
                <li><a href="#links">Links</a></li>
                <li><a href="#methods">Methods</a></li>
                <li><a href="#results">Results</a></li>
                <li><a href="#credits">Credits</a></li>
            </ul>
        </div>
    </nav>

    <section id="introduction">
        <div class="container">
            <h2>Introduction</h2>
            <p>Monopoly is a complex game with many random variables and complicated decision processes. Its complex dynamics, involving property acquisition, resource management, and player interaction, make it an ideal testbed for artificial intelligence (AI) techniques. In addition to the traditional fixed-policy strategy, there is also a rise in exploring reinforcement learning algorithms in games like this --- among which Q-learning is a simple and adaptive example.</p>

            <p>This project builds and compares three types of Q-learning agents for monopoly: the basic Q-table approach, approximate Q-learning, and deep Q-lambda learning. The basic Q-learning approch is simple to code and quick to train, but it may not be able to capture the complex dynamics of the game; the deep Q-lambda agent considers complex situations in the game and long-term rewards, but takes exceptionally long to train; the approximate Q-learning agent lies in-between, considering the complex dynamics but not long-term rewards, and trains in a moderate amount of time. With these variants we aim to explore the balance between efficiency, simplicity of code, and the ability to adapt to the large state-action space and the need for long-term strategic planning.</p>

            <p>Our agents were trained using <a href = "https://github.com/gamescomputersplay/monopoly">gamescomputersplay/monopoly</a>, a monopoly game simulator available on github. Our agents played fixed-policy players under simple and complex game conditions. The agents use a reward function that balances its property (land, houses) and money. Each agent was able to stablize and perform better than a random action agent. For demonstration purposes, the deep Q-lambda agent was exported using <a href = "https://github.com/onnx/onnx">Open Neural Network Exchange</a> and incorporated into <a href = "https://github.com/intrepidcoder/monopoly">intrepidcoder/monopoly<a>, a runtime JavaScript-based monoply game board that allows you to play with our agent.</p>
        </div>
    </section>

    
    <section id="links">
        <div class="container">
            <h2>See Our Work</h2>
            <ul>
                <li><a href="https://github.com/Comps-Monopoly-CarletonCSF24/MonopolyRL">Visit Our Github Repository</a></li>
                <li><a href="https://comps-monopoly-carletoncsf24.github.io/MonopolyRL/JavaScript%20Engine/">Play Against Our Deep Q-Lambda Agent</a></li>
            </ul>
        </div>
    </section>

    <section id="methods">
        <div class="container">
            <h2>Methods</h2>
            <h3>Q-learning Basics</h3>
            <p>Q-learning is a specific type of reinforcement learning (RL). In the reinforcement learning process, the agent observes a set of <b>states</b> that describe the environment, then takes an <b>action</b> to affect the environment. Finally, it receives <b>rewards</b> of how well that action improved the situation.</p>
            <image src= "./presentation/Reinforcement Learning basic.png" alt-text = "reinforcement larning">
            <p>Q-learning relies on the idea of <b>Q values</b>, the Q value: <i>Q(i, j) = q</i> represents that the agent's best estimate of the reward for taking action <i>j</i> at state <i>i</i> is <i>q</i>.</p>
            <p>Each turn, the agent decides what action to take based on the <b>epsilon-greedy</b> exploration policy. Essentially, it rolls a dice and checks if it is greater than <b>epsilon</b>, the exploration rate. If not, it <b>explores</b> the environment by taking a random action. Otherwise, it <b>exploits</b> its current knowledge of the environment by taking the action wiht the highest <b>Q value</b> at the current state.
            <p>After the action is taken, the agent calculates a <b>Temporal Difference</b> based on the reward after that action. This value represents how off the previous estmted Q value was and is used to update the <b>Q values</b>.</p>
            <p>The Q-learning agent plays a large number of games with fixed-policy players. In this <b>training</b> process, the <b>Q values</b> are constantly updated to become more and more precise, making the agent play better and better.
            <image src= "./presentation/q-learning cycle.png" alt-text = "Q-learning cycle">
            
            
            <h3>Fixed Policy Agent</h3>
            <h3>Basic Q Learning</h3>
            <h3>Approximate Q-learning</h3>
            <p> Approximate Q-Learning is a version of Q-Learning designed to handle large or continuous state spaces where storing every possible state-action pair isnâ€™t practical. In basic Q-Learning, you keep a table that records the Q-value for each state-action pair, which works well for small problems but quickly becomes unmanageable as the state space grows. Approximate Q-Learning solves this by using a function to estimate Q-values instead of storing them directly. The Q-value for a state-action pair is calculated using a linear approximation function with a set of weights. As the agent learns, it updates these weights based on the difference between predicted and actual Q-values. This approach allows the agent to generalize across similar states, making it more efficient and scalable for complex environments.</p>

            <div style="display: flex; align-items: center; margin-bottom: 20px;">
                <img src="temporal_diff_error.png" alt="Temporal Difference Error" style="width: 700px; height: auto; margin-right: 20px;">
                <div>
                    <h3>Temporal Difference Error</h3>
                    <p>The temporal difference error represents the difference between the predicted Q-value and the actual reward plus the discounted future value. The discount factor allows to decide if we prioritize current or future rewards during the learning.</p>
                </div>
            </div>
            
            <div style="display: flex; align-items: center;">
                <img src="weights.png" alt="Updating Weights" style="width: 1500px; height: auto; margin-right: 20px;">
                <div>
                    <h3>Updating Weights</h3>
                    <p>The model updates its weights using the temporal difference error to minimize future prediction errors and improve the Q-value approximation over time. The learning rate allows the model to decide how much of the newly learned information is taken into account and features are functions of the state s and action a. They translate complex state-action information into a more manageable set of values that the model can work with.</p>
                </div>
            </div>
            
            <h3>Deep Q-lambda Learning</h3>
            

        </div>
    </section>

    <section id="implementation">
        <div class="container">
            <h2>Implementation</h2>
            <h3>State Space: Approx Q and Deep Q</h3>
            The Approximate Q-earning agent and the Deep Q-lambda agent use a very complicated state space, borrowed from Bailis et al. (2014). Each state is a 23-dimensional vector containing:
            <ul>
                <li><b>10 dimensions representing the property status of the Q-learning agent.</b> Each color on the board (including rail and facilities) takes 1 dimension. The property status is projected to a fraction of a multiple of 1/17, in which 12/17 represents owning the land, and the rest represents owning houses.</li>
                <li><b>10 dimensions representing the property status of all other players combined.</b> In the same fashion as described above.</li>
                <li><b>1 dimension representing the position of the agent on the board</b>, which is a multiple of 1/40</li>
                <li><b>2 dimensions representing the general finance situation of the agent.</b> Onedimension is the fraction representing the ratio of the total number of property owned by the agent versus the total number of property owned by other players. The other dimension is a sigmoid function applied to the current amount of money the agent has.</li>
            </ul>
            <h3>Action Space</h3>
            The action space also refrerences Bailis et al. (2014). Each turn, the agent decides to take one of the following actions for each color group:
            <ul>
                <li><b>Buy</b>: If there are properties or houses to unmortgage, un mortegage one of them. Else, if you can buy a property in the current color group, then buy one. Otherwise, if you can build in the color group, build one house.</li>
                <li><b>Sell</b>: If there are houses to sell in the color group, sell one. Otherwise, mortgate one piece of land in the color group if possible.</li>
                <li><b>Do Nothing</b></li>
            </ul>
        </div>
    </section>

    <section id="results">
        <div class="container">
            <h2>Results</h2>
            <h3>Overall Results: Stablization and Survival Rate</h3>
            <table border="1" cellpadding="10" cellspacing="0">
                <thead>
                  <tr>
                    <th>Algorithm</th>
                    <th>Training Condition</th>
                    <th># Games to Stabilize</th>
                    <th>Survival Rate Against Fixed Policy Agent (Under the training condition)</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>Basic Q</td>
                    <td>2 players<br>No Trading<br>Long games</td>
                    <td>~180</td>
                    <td>
                      Random: 17.5%<br>
                      <b>BasicQ Agent: 54.8%</b><br>
                      Fixed Policy Agent: 66%
                    </td>
                  </tr>
                  <tr>
                    <td>Approximate Q</td>
                    <td>2 Players<br>No Trading<br>Long Games</td>
                    <td>~750</td>
                    <td>
                      Random: 17.5%<br>
                      <b>ApproxQ Agent: 65%</b><br>
                      Fixed Policy Agent: 66%
                    </td>
                  </tr>
                  <tr>
                    <td>Deep Q-Lambda</td>
                    <td>4 Players<br>With Trading<br>Short Games</td>
                    <td>~700</td>
                    <td>
                      Random: 20%<br>
                      <b>DQL Agent: 40%</b><br>
                      Fixed Policy Agent: 60%
                    </td>
                  </tr>
                </tbody>
              </table>
            
            <h3>Training Cycle: Deep Q-Lambda Learning</h3>
            <img src="./training_results/DQ.png" alt="Training results graph for Deep Q-Lambda Learning">
            <h3>Training Cycle: Approximate Q-Learning</h3>
            <img src="./training_results/approx.png" alt="Training results graph for approximate Q-Learning" style="width: 800px; height: 500px;">
        </div>
    </section>

    <section id="credits">
        <div class="container">
            <h2>Credits</h2>
            <ul>
                <li>The Python Monopoly engine was borrowed from gamescomputersplay/monopoly, available on GitHub.</li>
                <li>The JavaScript (web runtime) Monopoly Eengine is borrowed from intrepidcoder/monopoly, available on GitHub.</li>
                <li>The Deep Q-Lambda Agent code references pmpailis/rl-monopoly, available on GitHub</li>
                <li>The construction of the state-actions spaces, the reward function, and the update process of the Deep Q-Lambda Agent references the paper <i>Learning to play monopoly: A reinforcement learning approach.</i> by Bailis, P., Fachantidis, A., & Vlahavas, I. (2014)</li>
                <li>We would like to thank Dave Musicant, our project advisor, for his help and advice.</li>
                <li>We would like to thank the Computer Science department at Carleton College for making this experience possible.</li>
            </ul>
        </div>
    </section>

    <footer>
        <div class="container">
            <p>&copy; [Year] [Your Name]. All rights reserved.</p>
            <p>Contact: <a href="mailto:youremail@example.com">youremail@example.com</a></p>
        </div>
    </footer>

    <script>
        // Smooth scrolling for anchor links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });
    </script>
</body>
</html>